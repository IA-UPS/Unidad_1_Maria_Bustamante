---
title: "Resumen Cap 2/3"
format: pdf
editor: visual
---

**Integrantes:** María José Bustamante - Nicolás Jadán

**Carrera:** Biomedicina

# CAPÍTULO 2

# Aprendizaje estadístico

## ¿Qué es el aprendizaje estadístico?

Ayuda a determinar si existe una relación entre las variables de interés para su futura aplicación.

Variable de entrada (input): Suelen denotarse como "X" y con un subídice para diferenciarlas. Se conocen como; predictores, características y variables independientes.

Variable de salida (output): Se denota como "Y". Conocida generalmente como variable dependiente o de respuesta.

El aprendizaje estadístico se refiere a un conjunto de enfoques para estimar *f* que es una función fija pero desconocida que representa información sistemática que brinda X acerca de Y.

### ¿Cómo estimar *f*?

**Predicción**

Generalmente se suele tener los datos de entrada pero la parte difícil es obtener la salida. Esto se puede solucionar usando:

![](images/predicci%C3%B3n.png){fig-align="center" width="82" height="34"}

f: estimación para f

y: predicción resultante

Un ejemplo se muestra en la siguiente imagen:

![](images/imagen%20prediccion.png){fig-align="center" width="283"}

El gráfico muestra los ingresos en función de los años de educación y antigüedad en el conjunto de datos Renta. La superficie azul representa la verdadera relación subyacente entre los ingresos y los años de educación y antigüedad, que se conoce porque los datos son simulados. Los puntos rojos indican los valores de estas cantidades para 30 individuos.

La exactitud de y depende de dos tipos de error:

-   Error reducible: Se refiere a la precisión de las predicciones que puede mejorar implementando mejores algoritmos para estimar f(X).

-   Error irreducible: Aunque fuera posible obtener la mejor estimación de f(X), seguirá existiendo un cierto nivel de incertidumbre ya que siempre existen dependencias de la variable objetivo con otras variables que no se están considerando o simplemente por procesos debidos al azar. Esto es lo que se conoce como error irreducible. Siempre proporcionará un límite superior en la precisión de la predicción para Y.

**Inferencia**

Para comprender la asociación entre Y y Xp se pueden plantear las siguientes preguntas:

1.  *¿Qué predictores se asocian a la respuesta?*

2.  *¿Cuál es la relación entre la respuesta y cada predictor?*

3.  *¿Puede resumirse adecuadamente la relación entre Y y cada predictor mediante una ecuación lineal, o la relación es más complicada?*

Algunos modelos podrían utilizarse tanto para la predicción como para la inferencia, dependiendo delobjetivo final. Por ejemplo, los modelos lineales permiten una inferencia relativamente sencilla y predecible entre modelos lineales, pero puede que no produzcan predicciones tan precisas como otros enfoques. Por el contrario, algunos de los enfoques no lineales pueden brindar información bastante precisas para Y.

## ¿Cómo estimamos f?

**Métodos paramétricos**: Abarcan un enfoque basado en modelos de dos pasos:

1.  Suposición sobre la forma funcional para el modelo. Ej: f es lineal en X.

2.  Después de seleccionar el modelo se requiere de un procedimiento que utilice los datos de entrenamiento para ajustar o entrenar el modelo.

El método más común para ajustar un modelo es el de mínimos cuadrados.

Ejemplo:

![*Modelo lineal ajustado por mínimos cuadrados a datos de ingresos. Las observaciones se muestran en rojo, y el plano amarillo indica el ajuste por mínimos cuadrados a los datos.*](images/lineal%20mod.png){fig-align="center" width="377"}

El modelo paramétrico reduce el problema para estimar f ya que se presenta como un conjunto de parámetros en el modelo lineal caso contario se debería ajustar f a una función arbitraria.

La desventaja de este modelo es que cuando el modelo elegido se aleje demasiado la estimación será deficiente y para resolver esto se debería estimar una mayor cantidad de parámetros lo que a su vez podría provocar un sobreaajuste de datos.

**Métodos no paramétricos:** Buscan una estimación de f que se acerque lo más posible a los puntos de datos sin que sea demasiado aproximada. Se ajustan fácilmente a una gama más amplia de formas posibles de f.

Una desventaja que presentan es que se necesita un gran número de observaciones para obtener una estimación precisa de f.

## El equilibrio entre la precisión de la predicción y la interpretabilidad del modelo

Existen métodos menos flexible o menos restrictivos en el sentido de que sólo pueden producir una gama relativamente pequeña de formas para estimar f.

![*Representación de la relación entre flexibilidad e interpretabilidad, utilizando diferentes métodos de aprendizaje estadístico. En general, a medida que aumenta la flexibilidad de un método, disminuye su interpretabilidad.*](images/flexibilidad.png){fig-align="center" width="467"}

*Modelo restrictivo:* Es útil cuando el principal interés es la inferencia, debido a que es más interpretable.

*Modelos flexibles:* Pueden guiar a estimaciones muy complicadas de f en las que es difícil comprender cómo se asocia cualquier predictor con la respuesta.

-   lasso: es un enfoque menos flexible y más interpretable que la regresión lineal proque en el modelo final la respuesta sólo estará relacionada con el modelo final.

-   Modelos aditivos generalizados (GAM): Más felxibles que la regresión lineal, pero menos interpretables ya que la relación entre cada predictor y respuesta se representa mediante una curva.

-   Modelos no lineales: bagging, boosting, máquinas de soporte de vectores y redes neuronales.

## Aprendizaje supervisado frente a aprendizaje no supervisado

Supervisado: Para cada observación de los predictores hay una respesta asociada. Permite predecir con exactitud la prespuesta para futuras predicciones o comprender mejor la relaxción entre predictores y respuesta.

No supervisado: Se carece de una variable de respuesta que pueda supervisar el análisis. Es decir no hay una respuesta asociada al predictor, por lo que no e sposible ajustar a un modelo de regresión lineal.

## Problemas de regresión frente a problemas de clasificación

Variables cuantitativas: Toman valores numéricos. Ej: Estatura, edad o ingresos.

Variables cualitativas: Toman valores en clases o categorías. Ej: Estado cívil, marcas de productos o diagnósticos.

La regresión logística es un método de clasificación binaria. Es bastante común seleccionar los métodos de aprendizaje estadístico en función de si la respuesta es cuantitativa o cualitativa, es decir, se puede usar la regresión lineal cuando es cuantitativa y la regresión logística cuando es cualitativa.

# Evaluación de la precisión de los modelos

En en estadística: ningún método domina a todos los demás en todos los conjuntos de datos posibles.

## Medir la calidad del ajuste

Para evaluar el rendimiento de un método de aprendizaje estadístico se requiere cuantificar hasta qué punto el valor de respuesta predicho para una observación dada se aproxima al valor de respuesta verdadero para esa observación.

*Error cuadrático medio (MSE):* será pequeño si las respuestas predichas están muy cerca de las respuestas verdaderas, y será grande si para algunas de las observaciones, las respuestas predichas y verdaderas difieren sustancialmente.

Se calcula utilizando los datos de entrenamiento que se usaron para jaustar el modelo

![***Izquierda:** Datos simulados a partir de f, mostrados en negro. Se muestran tres estimaciones de f: la línea de regresión lineal (curva naranja) y dos splines de suavizado (curvas azul y verde). (curvas azul y verde). **Derecha:** MSE de entrenamiento (curva gris), MSE de prueba (curva roja roja) y el MSE de prueba mínimo posible de todos los métodos (línea discontinua). Los cuadrados MSE de entrenamiento y prueba de los tres ajustes mostrados en el panel izquierdo.*](images/MSE.png){fig-align="center" width="481"}

En la imagen en la parte izquierda se puede ver que a medida que aumenta el nivel de flexibilidad, las curvas se ajustan mejor a los datos observados. La curva verde es la más flexible y se ajusta muy bien a los datos; sin embargo, observamos que se ajusta mal a la f verdadera (mostrada en negro) porque es demasiado ondulada.

En la parte derecha La curva gris muestra el MSE medio de entrenamiento en función de la flexibilidad, o más formalmente de los grados de libertad (flexibilidad de la curva). Los cuadrados naranja, azul y verde indican los MSE asociados a las curvas.

En este ejemplo, la verdadera f no es lineal, por lo que el ajuste lineal naranja no es lo suficientemente flexible para estimar bien f. La curva verde tiene el MSE de entrenamiento más bajo de los tres métodos, ya que corresponde al más flexible de ellos. El spline de suavizado representado por la curva azul se aproxima al óptimo.

En la parte derecha de la figura, a medida que aumenta la flexibilidad del método de aprendizaje estadístico, observamos un descenso monótono en el tiempo de entrenamiento. Es decir, a medida que aumenta la flexibilidad del modelo aumenta, el MSE de entrenamiento disminuirá.

Cuando un método determinado produce un MSE de entrenamiento pequeño pero un MSE de prueba grande, se dice que se están sobreajustando los datos.

## La relación entre sesgo y varianza

Para minimizar el error de prueba esperado se requiere seleccionar un método de aprendizaje estadístico que consiga simultáneamente baja varianza y bajo sesgo.

Varianza: Se refire a la cantidad en la que *f* cambiaría si la estimación se realizara usando un conjunto de datos de enttrenamiento diferente. Lo ideal es que la estimación de f no varíe demasiado. En general los métodos estadísticos más flexibles tienen mayor varianza.

Sesgo: Es el error que se introduce al aproximar un problema de la vida real a un modelo muy simple o sencillo. Es poco probable que un problema de la vida real tenga una relación lineal tan sencilla, por lo que realizar una regresión lineal dará lugar a cierto sesgo en la estimación de f.

La varianza es intrínsecamente una cantidad no negativa, y el sesgo al cuadrado también es no negativo.

A medida que utilicemos métodos más flexibles, la varianza aumentará y el sesgo disminuirá.

Un buen rendimiento del conjunto de prueba de un método de aprendizaje estadístico requiere una varianza baja, así como un equilibrio entre sesgo y varianza.

## El entorno de clasificación

Las tasas de error resultantes son de especial interés para la aplicación del clasificador a observaciones de prueba que no fueron utilizadas en el entrenamiento. Un buen clasificador es aquel en el que el error de prueba es mínimo.

### El clasificador de Bayes

Asigna cada observación a la clase más probable dados sus valores predictores. En un problema de dos clases en el que sólo hay dos posibles valores de respuesta, el clasificador de Bayes corresponde a la predicción de la clase uno si Pr(Y = 1\|X = x0) \> 0,5, y la clase dos en caso contrario.

![*Un conjunto de datos simulados compuesto por 100 observaciones en cada uno de dos grupos, indicados en azul y en naranja. La línea discontinua morada representa el límite de decisión de Bayes. La cuadrícula de fondo naranja indica la región en la que una observación de prueba se asignará a la clase naranja, y la cuadrícula de fondo azul azul indica la región en la que una observación de prueba se asignará a la clase azul. a la clase azul.*](images/bayes.png){fig-align="center" width="411"}

En la imagen los círculos naranja y azules corresponden a observaciones de entrenamiento que pertenecen a dos clases diferentes.

Para cada valor de X1 y X2, existe una probabilidad diferente de que la respuesta sea naranja o azul.

La región sombreada en naranja refleja el conjunto de puntos para los que Pr(Y = naranja\|X) es superior al 50 %, mientras que la región sombreada en azul indica el conjunto de puntos cuya probabilidad es inferior al 50 %.

La línea discontinua morada representa los puntos en los que la probabilidad es exactamente del 50 %. Esto se denomina el **límite de decisión de Bayes.**

### K-Nearest Neighbors

Es utilizado para trabajar con datos reales. Es un algoritmo no supervisado donde "K" representa el número de "grupos" (clusters) a clasificar y el K-neighbor más cercano "K" representa el número de "vecinos" considerados en los "n" grupos del clasificador. En otras palabras busca en las observaciones más cercanas a la que se está tratando de predecir y clasifica el punto de interés basado en la mayoría de datos que le rodean.

![*Comparación de los límites de decisión KNN (curvas negras obtenidas utilizando K = 1 y K = 100 en los datos de la Figura 2.13. Con K = 1, el límite de decisión es demasiado flexible, mientras que con K = 100 no es suficientemente flexible. El límite de decisión de Bayes se muestra como una línea discontinua morada.*](images/k-neig.png){fig-align="center" width="446"}

La imagen muestra dos ajustes KNN a los datos simulados, utilizando K = 1 y K = 100. Cuando K = 1, el límite de decisión es excesivamente flexible y encuentra patrones en los datos que no se corresponden con el límite de decisión de Bayes. Esto corresponde a un clasificador que tiene un sesgo bajo pero una varianza muy alta. A medida que K aumenta, el método se vuelve menos flexible y produce una frontera de decisión cercana a la lineal. Esto corresponde a un clasificador de baja varianza pero alto sesgo. En este conjunto de datos simulados, ni K = 1 ni K = 100 dan buenas predicciones: tienen tasas de error de prueba de 0,1695 y 0,1925, respectivamente.

# CAPÍTULO 3

# Regresión linear

La regresión lineal, es un enfoque simple pero poderoso para el aprendizaje supervisado. El capítulo cubre las ideas clave que subyacen al modelo de regresión lineal, así como el enfoque de mínimos cuadrados que se usa más comúnmente para ajustar este modelo. También analiza algunas preguntas importantes que podríamos tratar de abordar al analizar datos y hacer recomendaciones para planes de marketing basados en análisis estadísticos. En general, el capítulo sirve como base para métodos de aprendizaje estadístico más avanzados y proporciona una descripción general de cómo se puede usar la regresión lineal para predecir respuestas cuantitativas.

## Regresión linear simple

La regresión lineal simple es un método estadístico utilizado para predecir una variable de respuesta cuantitativa basada en una única variable predictora. Asume que existe una relación aproximadamente lineal entre el predictor y las variables de respuesta. La relación lineal se puede expresar matemáticamente como Y ≈ β0 + β1X, donde Y es la variable de respuesta, X es la variable predictora, β0 y β1 son coeficientes que representan la intersección y la pendiente de la línea, respectivamente. El método se usa comúnmente en varios campos, como marketing, finanzas y ciencias sociales, para analizar datos y hacer predicciones.

### 

Estimating the Coefficients

Estimar los coeficientes es un paso crucial en la regresión lineal simple. En la práctica, los coeficientes β0 y β1 son desconocidos, por lo que antes de que podamos usar el modelo de regresión lineal para hacer predicciones, debemos usar datos para estimar estos coeficientes. Se proporciona una discusión detallada sobre cómo estimar estos coeficientes usando el enfoque de mínimos cuadrados. El enfoque de mínimos cuadrados implica encontrar los valores de β0 y β1 que minimizan la suma de los residuos cuadrados entre los valores predichos y reales de la variable de respuesta.

![FIGURA 1. Para los datos de publicidad, los mínimos cuadrados se ajustan a la regresión de las ventas en TV se muestra. El ajuste se obtiene minimizando la suma residual de cuadrícula. Cada segmento de línea gris representa un residuo. En este caso un ajuste lineal capta la esencia de la relación, aunque sobrestima la tendencia en la izquierda de la parcela.](images/reg.jpg)

### Evaluación de la precisión de las estimaciones del coeficiente

Es un paso importante en la regresión lineal simple. Una vez que hemos estimado los coeficientes β0 y β1, es natural querer cuantificar hasta qué punto estas estimaciones son precisas. presenta una discusión detallada sobre cómo calcular los errores estándar, las estadísticas t y los valores p para estas estimaciones de coeficientes. Estas estadísticas se pueden utilizar para comprobar si existe una relación lineal significativa entre las variables predictoras y de respuesta, así como para construir intervalos de confianza para los coeficientes. Se analiza cómo interpretar estas estadísticas y cómo se pueden usar para hacer inferencias sobre los parámetros de la población. En general, evaluar la precisión de las estimaciones de los coeficientes es un paso esencial en la regresión lineal simple, ya que nos permite determinar si nuestro modelo es estadísticamente significativo y si nuestras estimaciones son confiables.

![Fig 3. Un conjunto de datos simulado. Izquierda: La línea roja representa la verdadera relación, f(X)=2+3X, que se conoce como la línea de regresión de población. El la línea azul es la línea de mínimos cuadrados; es la estimación de mínimos cuadrados para f(X) basada en los datos observados, mostrados en negro. Derecha: La línea de regresión de la población es nuevamente se muestra en rojo, y la línea de mínimos cuadrados en azul oscuro. En azul claro, diez menos Se muestran líneas cuadradas, cada una calculada sobre la base de un conjunto aleatorio separado de observaciones. Cada línea de mínimos cuadrados es diferente, pero en promedio, los mínimos cuadrados Las líneas están bastante cerca de la línea de regresión de la población.](images/fig4.jpg)

![TABLA 1. Para los datos de Publicidad, coeficientes del modelo de mínimos cuadrados para la regresión del número de unidades vendidas sobre el presupuesto de publicidad televisiva. Un aumento de \$1,000 en el presupuesto de publicidad televisiva se asocia con un aumento en las ventas de alrededor de 50 unidades. (Recuerde que la variable de ventas está en miles de unidades, y la La variable TV está en miles de dólares).](images/tabl1.jpg)

### Evaluar la precisión del modelo

Es un paso importante en la regresión lineal simple. Una vez que hemos rechazado la hipótesis nula a favor de la hipótesis alternativa, es natural querer cuantificar hasta qué punto el modelo se ajusta a los datos. La calidad de un ajuste de regresión lineal generalmente se evalúa utilizando dos cantidades relacionadas: el error estándar residual (RSE) y la estadística R-cuadrado (R2). El RSE mide la cantidad promedio que la variable de respuesta se desvía del valor predicho, mientras que R2 mide qué parte de la variabilidad en la variable de respuesta puede explicarse por la variable predictora.

#### Error estándar residual

El error estándar residual (RSE) es una medida de la cantidad promedio que la variable de respuesta se desvía del valor predicho en la regresión lineal simple. Se especifica como calcular RSE usando la fórmula RSE = sqrt(RSS/(n-2)), donde RSS es la suma residual de los cuadrados y n es el tamaño de la muestra. El RSE mide la variabilidad de la variable de respuesta que no está explicada por la variable predictora, y se puede utilizar para evaluar qué tan bien se ajusta nuestro modelo a los datos. Un RSE más pequeño indica un mejor ajuste, mientras que un RSE más grande indica un peor ajuste. Analiza cómo interpretar RSE y cómo se puede usar junto con otras estadísticas como R-squared para evaluar la precisión del modelo. Nos permite evaluar qué tan bien se ajusta nuestro modelo a nuestros datos.

![TABLA 2. Para los datos de Publicidad, más información sobre los mínimos cuadrados modelo para la regresión del número de unidades vendidas sobre el presupuesto de publicidad televisiva.](images/tab2.jpg)

## Regresión lineal múltiple

La regresión lineal múltiple es una extensión de la regresión lineal simple que nos permite modelar la relación entre una variable de respuesta y varias variables predictoras. Se muestra cómo realizar una regresión lineal múltiple utilizando el enfoque de mínimos cuadrados. El enfoque de mínimos cuadrados implica encontrar los valores de β0, β1, β2, \..., βp que minimizan la suma de los residuos cuadrados entre los valores predichos y reales de la variable de respuesta. Muestra como interpretar estos coeficientes estimados y cómo probar si son estadísticamente significativos mediante la prueba de hipótesis. Además, el capítulo cubre temas como la selección de modelos, la multicolinealidad y los efectos de interacción en la regresión lineal múltiple. En general, la regresión lineal múltiple es una herramienta poderosa para modelar relaciones complejas entre variables y se puede utilizar en una amplia gama de aplicaciones en diversos campos, como la economía, las finanzas y las ciencias sociales.

![Tabla 3. Modelos de regresión lineal más simples para los datos publicitarios. Coeficientes del modelo de regresión lineal simple para el número de unidades vendidas en Top: presupuesto de publicidad en radio y Abajo: presupuesto de publicidad en periódicos. Un aumento de \$1,000 en el gasto en publicidad por radio está asociado con un aumento promedio en ventas en alrededor de 203 unidades, mientras que el mismo aumento en el gasto en publicidad en periódicos está asociado con un aumento promedio en las ventas de alrededor de 55 unidades. (Nota que la variable de ventas está en miles de unidades, y la radio y el periódico las variables están en miles de dólares).](images/tbl%203.jpg)

### Estimación de los coeficientes de regresión

Se analiza cómo estimar los coeficientes de regresión en Regresión lineal simple utilizando el enfoque de mínimos cuadrados. El enfoque de mínimos cuadrados implica encontrar los valores de β0 y β1 que minimizan la suma de los residuos cuadrados entre los valores predichos y reales de la variable de respuesta. El capítulo proporciona una discusión detallada sobre cómo calcular estas estimaciones de coeficientes usando fórmulas y cómo interpretarlas en el contexto de nuestros datos. Además, el capítulo cubre temas como errores estándar, estadísticas t y valores p para estas estimaciones de coeficientes. Estas estadísticas se pueden utilizar para evaluar la precisión del modelo y construir intervalos de confianza para los coeficientes. En general, estimar los coeficientes de regresión es un paso esencial en la regresión lineal simple, ya que nos permite hacer predicciones basadas en nuestros datos y evaluar qué tan bien se ajusta nuestro modelo a nuestros datos.

![FIGURA 4. En un entorno tridimensional, con dos predictores y una respuesta, la línea de regresión de mínimos cuadrados se convierte en un plano. El avión es elegido para minimizar la suma de las distancias verticales al cuadrado entre cada observación (mostrado en rojo) y el avión.](images/sd.jpg)

### ![TABLA 4. Para los datos de publicidad, las estimaciones del coeficiente de mínimos cuadrados de la regresión lineal múltiple del número de unidades vendidas en televisión, radio y periódicos presupuestos publicitarios](images/te4.jpg)

![Table 5.Matriz de correlación de TV, radio, periódico y ventas para el Datos publicitarios.](images/sss.jpg)

### Algunas preguntas importantes

Cuando realizamos una regresión lineal múltiple, por lo general estamos interesados ​​en respondiendo algunas preguntas importantes. 1. ¿Al menos uno de los predictores X1, X2,\...,Xp es útil para predecir ¿la respuesta? 2. ¿Todos los predictores ayudan a explicar Y , o es solo un subconjunto del predictores útil? 3. ¿Qué tan bien se ajusta el modelo a los datos? 4. Dado un conjunto de valores predictores, ¿qué valor de respuesta deberíamos predecir? y ¿qué tan precisa es nuestra predicción? Ahora abordaremos cada una de estas preguntas por separado.

\
Uno: ¿Existe una relación entre la respuesta y los predictores?

Dos: decidir sobre variables importantes\
Tres: ajuste del modelo Cuatro: predicciones

## Otras consideraciones en el modelo de regresión

### Predictores cualitativos

En el contexto de la regresión lineal, los predictores cualitativos son variables que toman valores no numéricos, como categorías o etiquetas. Por ejemplo, en el conjunto de datos de Crédito que se muestra en la Figura 6, la variable predictora "educación" representa los años de educación y es una variable cuantitativa, mientras que la variable predictora "calificación" representa la calificación crediticia y es una variable cualitativa. Cuando se trata de predictores cualitativos en regresión lineal, necesitamos convertirlos en variables numéricas mediante un proceso llamado codificación. Un método común para codificar predictores cualitativos se llama codificación ficticia, donde creamos variables binarias para representar cada categoría del predictor cualitativo.

![FIGURA 6. El conjunto de datos de crédito contiene información sobre saldo, antigüedad, tarjetas, educación, ingresos, límite y calificación para una cantidad de clientes potenciales](images/sda.jpg)

### Extensiones del modelo lineal

El modelo de regresión lineal estándar hace varias suposiciones altamente restrictivas que a menudo se violan en la práctica. Dos de los supuestos más importantes establecen que la relación entre los predictores y la respuesta es aditiva y lineal. El supuesto de aditividad significa que la asociación entre un predictor Xj y la respuesta Y no depende de los valores de los otros predictores. La suposición de linealidad establece que el cambio en la respuesta Y asociado con un cambio de una unidad en Xj es constante, independientemente del valor de Xj.

![FIGURA 7. Para los datos de crédito, se muestran las líneas de mínimos cuadrados para la predicción del saldo de ingresos para estudiantes y no estudiantes. Izquierda: El modelo (3.34) estaba en forma. No hay interacción entre ingreso y estudiante. Derecha: El el modelo (3.35) se ajustaba. Existe un término de interacción entre ingreso y estudiante.](images/dsads.jpg)

Para abordar estas limitaciones, existen varias extensiones del modelo lineal, como la regresión polinomial, los efectos de interacción y los modelos lineales generalizados. La regresión polinomial nos permite modelar relaciones no lineales entre los predictores y la respuesta al incluir términos de predictores de orden superior en nuestro modelo. Los efectos de interacción nos permiten modelar cómo dos o más predictores interactúan entre sí para afectar la variable de respuesta. Los modelos lineales generalizados amplían la regresión lineal para manejar variables de respuesta no normales mediante el uso de una función de enlace para relacionar la media de la variable de respuesta con una combinación lineal de predictores.

Estas extensiones brindan más flexibilidad en el modelado de problemas del mundo real donde las relaciones entre predictores y respuestas pueden no ser estrictamente aditivas o lineales.

![FIGURA 8. El conjunto de datos automático. Para una cantidad de autos, mpg y caballos de fuerza son mostrado. El ajuste de regresión lineal se muestra en naranja. El ajuste de regresión lineal para un el modelo que incluye caballos de fuerza2 se muestra como una curva azul. La regresión lineal apto para un modelo que incluye todos los polinomios de caballos de fuerza hasta el quinto grado es se muestra en verde](images/dsad.jpg)

### Relaciones no lineales

La regresión lineal asume una relación lineal entre la respuesta y los predictores. Sin embargo, en algunos casos, la verdadera relación entre la respuesta y los predictores puede no ser lineal. Para dar cabida a las relaciones no lineales, podemos utilizar la regresión polinomial, que es una forma sencilla de extender directamente el modelo lineal. La regresión polinomial nos permite modelar relaciones no lineales entre los predictores y la respuesta al incluir términos de predictores de orden superior en nuestro modelo

### Problemas potenciales

#### 1. No linealidad de las relaciones respuesta-predictor

La no linealidad significa que la relación entre la variable de respuesta y una o más variables predictoras no es lineal, lo que puede conducir a estimaciones sesgadas o ineficientes de los coeficientes de regresión. Para abordar la no linealidad, podemos usar la regresión polinomial, que nos permite modelar relaciones no lineales entre los predictores y la respuesta al incluir términos de predictores de orden superior en nuestro modelo.

![](images/jj.jpg)

#### 

2.  Correlación de términos de error.

Una suposición importante del modelo de regresión lineal es que los términos de error, ε1, ε2, \..., εn, no están correlacionados. Esto significa que si los errores no están correlacionados, el hecho de que εi sea positivo proporciona poca o ninguna información sobre el signo de εi+1. Los errores estándar que se calculan para los coeficientes de regresión estimados o los valores ajustados se basan en la suposición de términos de error no correlacionados. Si de hecho existe una correlación entre los términos de error, entonces los errores estándar estimados tenderán a subestimar los errores estándar verdaderos.

![FIGURA 10. Gráficas de residuos de conjuntos de datos de series de tiempo simulados generados con diferentes niveles de correlación ρ entre términos de error para puntos de tiempo adyacentes.](images/fr.jpg)

#### ![FIGURA 11. Parcelas residuales. En cada parcela, la línea roja es un ajuste suave a la residuales, destinados a facilitar la identificación de una tendencia. Las líneas azules rastrean el cuantiles externos de los residuales y enfatizar patrones. Izquierda: La forma de embudo indica heterocedasticidad. Derecha: la respuesta se ha transformado logarítmicamente y ahora no hay evidencia de heteroscedasticidad.](images/gres.jpg)

3.  Varianza no constante de los términos de error.

Otra suposición importante del modelo de regresión lineal es que los términos de error tienen una varianza constante, Var(εi)=σ2. Los errores estándar, los intervalos de confianza y las pruebas de hipótesis asociadas con el modelo lineal se basan en esta suposición. Si la varianza de los términos de error no es constante, los errores estándar estimados estarán sesgados y las pruebas de hipótesis y los intervalos de confianza no serán válidos. Una forma de abordar la varianza no constante es usar la regresión de mínimos cuadrados ponderados, que asigna pesos más grandes a las observaciones con varianzas más pequeñas y pesos más pequeños a las observaciones con varianzas más grandes.

#### ![FIGURA 12. Izquierda: La línea de regresión de mínimos cuadrados se muestra en rojo, y la la línea de regresión después de eliminar el valor atípico se muestra en azul. Centro: El residual la trama identifica claramente el valor atípico. Derecha: El valor atípico tiene un residuo estudentizado de 6; típicamente esperamos valores entre −3 y 3.](images/sdas.jpg)

4.  Valores atípicos.

Un valor atípico es un punto para el cual yi está lejos del valor predicho por el modelo. Los valores atípicos pueden surgir por una variedad de razones, como el registro incorrecto de una observación durante la recopilación de datos. Los valores atípicos pueden tener un gran efecto en los coeficientes de regresión estimados y pueden conducir a estimaciones sesgadas o ineficientes. Una forma de abordar los valores atípicos es utilizar métodos de regresión robustos, que son menos sensibles a los valores atípicos que la regresión ordinaria de mínimos cuadrados. Otro enfoque es identificar y eliminar los valores atípicos del conjunto de datos, aunque esto debe hacerse con precaución y solo después de una cuidadosa consideración de las razones de los valores atípicos.

#### ![FIGURA 13. Izquierda: la observación 41 es un punto de alto apalancamiento, mientras que la 20 no lo es. La línea roja es el ajuste a todos los datos, y la línea azul es el ajuste con la observación. 41 eliminados. Centro: la observación roja no es inusual en términos de su valor X1 o su valor X2, pero aún queda fuera de la mayor parte de los datos y, por lo tanto, tiene un alto aprovechar. Derecha: la observación 41 tiene un alto apalancamiento y un alto residual.](images/outl.jpg)

5.  Puntos de alto apalancamiento.

Los puntos de alto apalancamiento son observaciones con un valor inusual para xi. Estos puntos pueden tener un gran efecto en los coeficientes de regresión estimados y pueden conducir a estimaciones sesgadas o ineficientes. Una forma de identificar puntos de alto apalancamiento es calcular la estadística de apalancamiento, que mide la influencia de cada observación en los coeficientes de regresión estimados. Un punto de alto apalancamiento es aquel para el cual la estadística de apalancamiento es mucho mayor que el valor promedio de la estadística de apalancamiento para todas las observaciones. Para abordar los puntos de alto apalancamiento, podemos usar herramientas de diagnóstico como gráficos residuales y la distancia de Cook para identificar observaciones influyentes y potencialmente eliminarlas del conjunto de datos.

#### 6. Colinealidad.

La colinealidad se refiere a la situación en la que dos o más variables predictoras están estrechamente relacionadas entre sí. La colinealidad puede dificultar la estimación de los efectos individuales de cada variable predictora sobre la variable de respuesta y puede conducir a estimaciones inestables e ineficientes de los coeficientes de regresión. Una forma de detectar la colinealidad es calcular la matriz de correlación entre las variables predictoras. Un alto coeficiente de correlación entre dos variables predictoras indica que pueden ser colineales. Para abordar la colinealidad, podemos utilizar técnicas como la regresión de componentes principales o la regresión de crestas, que pueden ayudar a estabilizar las estimaciones de los coeficientes de regresión en presencia de predictores colineales.

![FIGURA 14. Diagramas de dispersión de las observaciones del conjunto de datos Credit. Izquierda: Una gráfica de edad versus límite. Estas dos variables no son colineales. Derecha: una trama de calificación versus límite. Hay alta colinealidad.](images/jh.jpg)

![FIGURA 15. Gráficos de contorno para los valores RSS en función de los parámetros β para varias regresiones que involucran el conjunto de datos Credit. En cada parcela, el negro los puntos representan los valores de los coeficientes correspondientes a la RSS mínima. Izquierda: Un gráfico de contorno de RSS para la regresión del equilibrio sobre la edad y el límite. El el valor mínimo está bien definido. Derecha: un gráfico de contorno de RSS para la regresión de equilibrio en calificación y límite. Debido a la colinealidad, hay muchos pares (βLimit, βRating) con un valor similar para RSS.](images/jh1.jpg)

![TABLA 11. Los resultados de dos modelos de regresión múltiple que implican la Se muestra el conjunto de datos de crédito. El modelo 1 es una regresión de equilibrio sobre edad y límite, y el Modelo 2 una regresión de equilibrio sobre calificación y límite. El error estándar de βˆlímite aumenta 12 veces en la segunda regresión, debido a la colinealidad.](images/te.jpg)

## El Plan de Mercadeo

Para responder a las siete preguntas sobre los datos publicitarios que nos propusimos responder al comienzo de este capítulo, necesitamos desarrollar un plan de marketing basado en el análisis de los datos. Un enfoque para desarrollar un plan de marketing es utilizar el análisis de regresión para modelar la relación entre el presupuesto de publicidad y las ventas de productos. Podemos usar técnicas como la regresión lineal múltiple o la regresión polinomial para capturar cualquier relación no lineal entre estas variables. También podemos utilizar herramientas de diagnóstico, como gráficos de residuos y pruebas de hipótesis, para evaluar la bondad de ajuste del modelo e identificar posibles problemas, como colinealidad o valores atípicos. Con base en los resultados de nuestro análisis, podemos desarrollar un plan de marketing que se adapte a las necesidades específicas de la empresa y su público objetivo.

1.  ¿Existe una relación entre las ventas y el presupuesto de publicidad?
2.  ¿Qué tan fuerte es la relación?
3.  ¿Qué medios están asociados con las ventas?
4.  ¿Qué tan grande es la asociación entre cada medio y las ventas?
5.  ¿Con qué precisión podemos predecir las ventas futuras?
6.  ¿La relación es lineal?
7.  ¿Existe sinergia entre los medios publicitarios?

## Comparación de regresión lineal con K-vecinos más cercanos

La sección presenta una comparación del rendimiento de la regresión lineal y KNN en conjuntos de datos con relaciones levemente no lineales y fuertemente no lineales entre las variables predictoras y las variables de respuesta. Los resultados muestran que KNN puede superar la regresión lineal en los casos en que la relación entre las variables predictoras y las variables de respuesta no es lineal. Sin embargo, la elección del valor de K puede tener un impacto significativo en el rendimiento de KNN y seleccionar un valor apropiado para K puede ser un desafío. En general, la elección entre regresión lineal y KNN depende de las características específicas del conjunto de datos y los objetivos del análisis.

![FIGURA 16. Gráficas de ˆf(X) usando la regresión KNN en datos bidimensionales conjunto con 64 observaciones (puntos naranjas). Izquierda: K = 1 da como resultado un ajuste de función de paso aproximado. Derecha: K = 9 produce un ajuste mucho más suave](images/holas.jpg)

![FIGURA 17. Gráficas de ˆf(X) usando regresión KNN en datos unidimensionales conjunto con 50 observaciones. La verdadera relación viene dada por la línea continua negra. Izquierda: La curva azul corresponde a K = 1 e interpola (es decir, pasa directamente a través de) los datos de entrenamiento. Derecha: La curva azul corresponde a K = 9, y representa un ajuste más suave.](images/ll.jpg)

![FIGURA 18. El mismo conjunto de datos que se muestra en la figura 3.17 se investiga más a fondo. Izquierda: la línea discontinua azul es el ajuste de mínimos cuadrados a los datos. Como f(X) está en hecho lineal (mostrado como la línea negra), la línea de regresión de mínimos cuadrados proporciona una muy buena estimación de f(X). Derecha: La línea horizontal discontinua representa el conjunto de prueba de mínimos cuadrados MSE, mientras que la línea continua verde corresponde al MSE para KNN en función de 1/K (en la escala logarítmica). La regresión lineal logra un MSE de prueba más bajo que la regresión KNN, ya que f(X) es de hecho lineal. Para KNN regresión, los mejores resultados se producen con un valor muy grande de K, correspondiente a un pequeño valor de 1/K.](images/le.jpg)

![FIGURA 19. Arriba a la izquierda: en un entorno con una relación ligeramente no lineal entre X e Y (línea negra continua), el KNN encaja con K = 1 (azul) y K = 9 (rojo) se muestran. Arriba a la derecha: para los datos ligeramente no lineales, el conjunto de prueba MSE para regresión de mínimos cuadrados (negro horizontal) y KNN con varios valores de Se muestran 1/K (verde). Inferior izquierda e inferior derecha: como en el panel superior, pero con una relación fuertemente no lineal entre X e Y.](images/l3.jpg)

![FIGURE 20. Test MSE for linear regression (black dashed lines) and KNN (green curves) as the number of variables p increases. The true function is non-- linear in the first variable, as in the lower panel in Figure 3.19, and does not depend on the additional variables. The performance of linear regression deteriorates slowly in the presence of these additional noise variables, whereas KNN's performance degrades much more quickly as p increases.](images/l4.jpg)

# LAB: CAP 3 - Regresión linear

Librerías a usar:

MASS: colección de conjunto de datos y funciones.

ISLR2: Conjunto de datos asociados al libro de estudio.

```{r}
library (MASS)
library (ISLR2)
```

## Regresión linear simple

La biblioteca ISLR2 contiene el conjunto de datos de Boston, que registra el medv (valor medio de la vivienda) de 506 secciones censales de Boston. Intentaremos predecir medv utilizando 12 predictores como rm (número medio de habitaciones por casa) edad (edad media de las casas) y lstat (porcentaje de hogares con un estatus socioeconómico bajo).

```{r}
head(Boston)
```

-   **?Boston:** Para obtener información sobre el conjunto de datos.

-   **lm():** Para ajustar un modelo de regresión lineal simple.

-   **Predictor:** Istat

-   **Respuesta:** medv

La sintaxis básica es lm(y ∼ x, datos), donde y es la respuesta, x es el predictor y datos es el conjunto de datos en el que se mantienen estas dos variables.

```{r}
attach(Boston)
lm.fit<-lm(medv~lstat,data = Boston)
lm.fit<-lm(medv~lstat)
```

Si escribimos lm.fit, aparecerá información básica sobre el modelo.

Para obtener información más detallada, utilice summary(lm.fit). Se obtienen los valores p y los errores estándar de los coeficientes, así como el estadístico R2 y el estadístico F del modelo.

```{r}
lm.fit
summary(lm.fit)
```

Podemos utilizar la función names() para averiguar qué otras piezas nombres de información se almacenan en lm.fit.

Es más seguro utilizar las funciones extractoras como coef() para acceder a ellas.

```{r}
names(lm.fit)
coef(lm.fit)
```

confint(): Para obtener un intervalo de confianza para las estimaciones de los coeficientes.

```{r}
confint(lm.fit)
```

predict(): Usada para producir intervalos de confianza para la predicción de medv para un valor dado de lstat.

```{r}
predict(lm.fit,data.frame(lstat=(c(5,10,15))),interval = "confidence")
predict(lm.fit,data.frame(lstat=(c(5,10,15))),interval = "prediction")
```

Por ejemplo, el intervalo de confianza del 95 % asociado a un valor lstat de 10 es (24.47, 25.63), y el intervalo de predicción del 95 % es (12.828, 37.28). Como era de esperar, los intervalos de confianza y predicción se centran en el mismo punto (un valor predicho de 10). En el mismo punto (un valor predicho de 25,05 para medv cuando lstat es igual a 10), pero estos últimos son mucho más amplios.

Ahora trazaremos medv y lstat junto con la línea de regresión por mínimos cuadrados utilizando las funciones plot() y abline(). **abline():** se puede usar para agregar líneas verticales, horizontales o de regresión a un gráfico.

```{r}
plot(lstat,medv)
abline(lm.fit)
```

Se puede observar indicios de no linealidad en la relación entre lstat y medv.

En la función abline() para dibujar una recta con intersección en a y pendiente b, se escribe **abline(a,b)**. Unos ajustes adicionales para trazar líneas y puntos son:

-   lwd: Para la anchura de la línea de regresión.

-   pch: Para crear diferentes símbolos de trazado.

```{r}
plot(lstat,medv)
abline (lm.fit , lwd = 3)
abline (lm.fit , lwd = 3, col = " red ")
plot (lstat , medv , col = " red ")
plot (lstat , medv , pch = 20)
plot (lstat , medv , pch = "+")
plot (1:20, 1:20, pch = 1:20)
```

par() y mfrow(): Indicaan a R que divida la pantalla de vizualización en paneles separados para poder visualizar gráficos simultaneamente.

```{r}
plot(lm.fit)
par(mfrow=c(2,2))
```

residuals (): Para calcular los residuos de un ajuste de regresión lineal.

rstudent(): Devuelve los residuales estudiados y se puede usar para representar gráficamente los residuos frente a los valores ajustados

```{r}
plot ( predict (lm.fit), residuals (lm.fit))
plot ( predict (lm.fit), rstudent (lm.fit))
```

hatvalues(): Sirve para calcular los estadísticos utilizando cualquier número de predictores.

which max: identifica el índice del elemento mayor de un vector. En este caso dice que la observación tiene el mayor estadístico de apalancamiento (375).

```{r}
plot ( hatvalues (lm.fit))
which.max( hatvalues (lm.fit))
```

## Regresión lineal múltiple

Para ajustar un modelo de regresión lineal múltiple por mínimos cuadrados, utilizamos de nuevo la función lm(). La sintaxis lm(y ∼ x1 + x2 + x3) se utiliza para ajustar un modelo con tres predictores, x1, x2 y x3.

```{r}
lm.fit<-lm(medv~lstat+age,data=Boston)
summary(lm.fit)
```

El conjunto Boston contiene 12 variables por lo que sería más tardado escribir todas las variables, en su lugar se utiliza:

```{r}
lm.fit<-lm(medv~.,data = Boston)
summary(lm.fit)
```

Para acceder a los componentes específicos de un elemento se escribe: `summary(lm.fit)$sigma`

vif(): utilizada para calcular los factores de inflación de la varianza. Para esto es necesario usar la siguiente librería:

```{r}
library(car)
vif(lm.fit)
```

Para utilizar todas la variables menos una:

```{r}
lm.fit1<-lm(medv~.-age,data = Boston)
summary(lm.fit1)
```

Como alternativa, puede utilizarse la función update()

```{r}
lm.fit1<-update(lm.fit,~.-age)
```

## Términos de interacción

lstat:black: Indica a R que debe incluir un término de interacción entre lstat y black.

lstat\*age: incluye simultaneamente lstat, age, y el término de interacción lstat x age como predictores.

```{r}
summary(lm(medv~lstat*age,data = Boston))
```

## Transformaciones no lineales de los predictores

lm puede ser ajustada a transformaciones no lineales de los predictores. La función I() es necesaria ya que el \^ tiene un significado especial I() en un objeto de fórmula; permite el uso estándar en R, que es elevar X a la potencia 2. Ahora realizamos una regresión de medv sobre lstat y lstat2.

```{r}
lm.fit2<-lm(medv~lstat+I(lstat^2))
summary(lm.fit2)
```

El valor p casi nulo asociado al término cuadrático sugiere modelo mejorado.

anova(): Sirve para cuantificar mejor hasta qué punto el ajuste cuadrático es superior al lineal.

```{r}
lm.fit<-lm(medv~lstat)
anova(lm.fit,lm.fit2)
```

Modelo 1: Representa el submodelo lineal que tiene un solo predictor (lstat).

Modelo 2: Modelo cuadrático más amplico con dos predictores (lstat y lstat\^2).

La función anova realiza una pruea de hipótesis que compara los dos modelos.

Una hipótesis nula quiere decir que los dos modelos se ajustan correctamente a los datos.

Una hipótesis alternativa significa que el modelo es superior.

En este caso, el estadístico F es 135 y el valor p asociado es prácticamente cero. Esto demuestra claramente que el modelo que contiene los predictores lstat y lstat2 es muy superior al modelo que sólo contiene el predictor lstat. Esto quiere decir que no existe linelidad.

```{r}
plot(lm.fit2)
par(mfrow=c(2,2))
```

Cuando se incluye lstat2 en el modelo se puede observar que hay pocos patrones dicernibles en los residuos.

Par crear un ajuste mayor se puede utilizar la función **poly()**.

```{r}
lm.fit5<-lm(medv~poly(lstat,5))
summary(lm.fit5)
```

Incluir términos de hasta quinto orden mejora el ajuste del modelo, pero si son mayores a este orden el modelo no tendrá valores p significativos en el ajuste de regresión.

Para obtener los polinomios brutos de la función poly(), debe utilizarse el argumento raw = TRUE.

**Tranformación logarítmica:**

```{r}
summary(lm(medv~log(rm),data = Boston))
```

## Predictores cualitativos

Para esta parte se usará: Carseats data para intentar predecir las ventas de asientos de autos en 4000 localidades basado en una serie de predictores.

```{r}
head(Carseats)
```

Predictores cualitativos incluidos: Shelveloc (calidad de la ubicación de la estantería). Tiene 3 valores posibles: Malo, medio y bueno. A continuación ajustamos un modelo de regresión múltiple que incluye algunos términos de interacción.

```{r}
lm.fit<-lm(Sales~.+Income:Advertising+Price:Age,data = Carseats)
summary(lm.fit)
```

contrasts() devuelve la codificación que R utiliza para las variables ficticias.

```{r}
attach(Carseats)
contrasts(ShelveLoc)
```

ShelveLocGood que toma un valor de 1 si la ubicación de la estantería es buena, y 0 en caso contrario.

El hecho de que el coeficiente de ShelveLocGood en el resultado de la regresión sea positivo indica que una buena estanterías se asocia a ventas elevadas (en comparación con las malas).

ShelveLocMedium tiene un coeficiente positivo menor, lo que indica tiene ubicación media de las estanterías se asocia a mayores ventas que a una mala ubicación, pero con menores ventas que una buena ubicación.

## Funciones de escritura

```{r}
LoadLibraries <- function () {
library (ISLR2)
library (MASS)
print ("The libraries have been loaded .")
}
```

Ahora si se escribe LoadLibraries, R dirá qué contiene la función.

```{r}
LoadLibraries
```

Si llamamos a la función, las librerías se cargan y la sentencia print se imprime.

```{r}
LoadLibraries()
```
